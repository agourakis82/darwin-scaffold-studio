"""
UncertaintyQuantification.jl - SOTA+++ Uncertainty Quantification Framework

Implements state-of-the-art uncertainty quantification methods:
- Bayesian Neural Networks (variational inference)
- Conformal Prediction (distribution-free calibrated intervals)
- Uncertainty Decomposition (aleatoric vs epistemic)
- Calibration Diagnostics

Created: 2025-12-21
Author: Darwin Scaffold Studio Team
Version: 3.4.0
"""

module UncertaintyQuantification

using Flux
using Statistics
using Random
using Distributions
using LinearAlgebra

export BayesianNN, ConformalPredictor, UncertaintyDecomposition
export train_bayesian!, predict_with_uncertainty, calibrate_conformal!
export predict_conformal, decompose_uncertainty, calibration_curve
export print_uncertainty_summary

# ============================================================================
# Bayesian Neural Network (Variational Inference)
# ============================================================================

"""
    BayesianNN

Bayesian Neural Network using variational inference (Bayes by Backprop).
Captures epistemic uncertainty through weight distributions.

# Fields
- `mean_net::Chain`: Mean network parameters
- `logvar_net::Chain`: Log-variance network parameters
- `prior_σ::Float32`: Prior standard deviation
- `n_samples::Int`: Number of MC samples for prediction
"""
mutable struct BayesianNN
    mean_net::Chain
    logvar_net::Chain
    prior_σ::Float32
    n_samples::Int
end

"""
    BayesianNN(input_dim, hidden_dims, output_dim; prior_σ=1.0f0, n_samples=100)

Construct a Bayesian Neural Network.

# Arguments
- `input_dim::Int`: Input dimension
- `hidden_dims::Vector{Int}`: Hidden layer dimensions
- `output_dim::Int`: Output dimension
- `prior_σ::Float32`: Prior standard deviation (default: 1.0)
- `n_samples::Int`: MC samples for prediction (default: 100)

# Example
```julia
bnn = BayesianNN(10, [64, 32], 1)
```
"""
function BayesianNN(input_dim::Int, hidden_dims::Vector{Int}, output_dim::Int; 
                    prior_σ::Float32=1.0f0, n_samples::Int=100)
    
    # Build mean network
    layers_mean = []
    prev_dim = input_dim
    for hdim in hidden_dims
        push!(layers_mean, Dense(prev_dim, hdim, relu))
        prev_dim = hdim
    end
    push!(layers_mean, Dense(prev_dim, output_dim))
    mean_net = Chain(layers_mean...)
    
    # Build log-variance network (same architecture)
    layers_logvar = []
    prev_dim = input_dim
    for hdim in hidden_dims
        push!(layers_logvar, Dense(prev_dim, hdim, relu))
        prev_dim = hdim
    end
    push!(layers_logvar, Dense(prev_dim, output_dim))
    logvar_net = Chain(layers_logvar...)
    
    return BayesianNN(mean_net, logvar_net, prior_σ, n_samples)
end

"""
    elbo_loss(bnn, x, y)

Evidence Lower Bound (ELBO) loss for variational inference.
ELBO = -KL(q(w)||p(w)) + E_q[log p(y|x,w)]
"""
function elbo_loss(bnn::BayesianNN, x, y)
    # Forward pass
    μ = bnn.mean_net(x)
    logσ² = bnn.logvar_net(x)
    σ² = exp.(logσ²)
    
    # Sample from variational posterior using reparameterization trick
    ε = randn(Float32, size(μ))
    y_pred = μ .+ sqrt.(σ²) .* ε
    
    # Reconstruction loss (negative log-likelihood)
    recon_loss = sum((y .- y_pred).^2) / size(y, 2)
    
    # KL divergence: KL(q(w)||p(w)) for Gaussian prior
    # KL = 0.5 * sum(σ² + μ² - 1 - log(σ²))
    kl_div = 0.5f0 * sum(σ² .+ μ.^2 .- 1.0f0 .- logσ²) / size(y, 2)
    
    # ELBO = reconstruction - KL (we minimize negative ELBO)
    return recon_loss + 0.01f0 * kl_div  # β=0.01 for KL annealing
end

"""
    train_bayesian!(bnn, X_train, y_train; epochs=100, lr=0.001)

Train Bayesian Neural Network using variational inference.

# Arguments
- `bnn::BayesianNN`: Bayesian neural network
- `X_train::Matrix`: Training inputs (features × samples)
- `y_train::Matrix`: Training targets (outputs × samples)
- `epochs::Int`: Number of training epochs (default: 100)
- `lr::Float64`: Learning rate (default: 0.001)

# Returns
- `losses::Vector{Float64}`: Training losses per epoch
"""
function train_bayesian!(bnn::BayesianNN, X_train::AbstractMatrix, y_train::AbstractMatrix;
                        epochs::Int=100, lr::Float64=0.001)
    
    # Use new Flux API - setup optimizer state
    opt_state = Flux.setup(Adam(lr), bnn.mean_net)
    opt_state_logvar = Flux.setup(Adam(lr), bnn.logvar_net)
    
    losses = Float64[]
    
    for epoch in 1:epochs
        loss_val = 0.0
        
        # Mini-batch training
        n_samples = size(X_train, 2)
        batch_size = min(32, n_samples)
        n_batches = max(1, div(n_samples, batch_size))
        
        for batch in 1:n_batches
            batch_start = (batch - 1) * batch_size + 1
            batch_end = min(batch * batch_size, n_samples)
            x_batch = X_train[:, batch_start:batch_end]
            y_batch = y_train[:, batch_start:batch_end]
            
            # Compute gradient and update using new API
            loss, grads = Flux.withgradient(bnn.mean_net) do m
                elbo_loss(BayesianNN(m, bnn.logvar_net, bnn.prior_σ, bnn.n_samples), x_batch, y_batch)
            end
            
            Flux.update!(opt_state, bnn.mean_net, grads[1])
            
            # Update logvar network
            loss_lv, grads_lv = Flux.withgradient(bnn.logvar_net) do lv
                elbo_loss(BayesianNN(bnn.mean_net, lv, bnn.prior_σ, bnn.n_samples), x_batch, y_batch)
            end
            
            Flux.update!(opt_state_logvar, bnn.logvar_net, grads_lv[1])
            
            loss_val += loss
        end
        
        push!(losses, loss_val / n_batches)
        
        if epoch % 10 == 0
            println("Epoch $epoch: Loss = $(round(losses[end], digits=4))")
        end
    end
    
    return losses
end

"""
    predict_with_uncertainty(bnn, X_test)

Predict with uncertainty using Monte Carlo sampling.

# Returns
- `mean::Vector`: Predictive mean
- `std::Vector`: Predictive standard deviation (total uncertainty)
- `samples::Matrix`: MC samples (for further analysis)
"""
function predict_with_uncertainty(bnn::BayesianNN, X_test::AbstractMatrix)
    n_test = size(X_test, 2)
    samples = zeros(Float32, bnn.n_samples, n_test)
    
    # Monte Carlo sampling
    for i in 1:bnn.n_samples
        μ = bnn.mean_net(X_test)
        logσ² = bnn.logvar_net(X_test)
        σ² = exp.(logσ²)
        
        # Sample from predictive distribution
        ε = randn(Float32, size(μ))
        samples[i, :] = vec(μ .+ sqrt.(σ²) .* ε)
    end
    
    # Compute statistics
    pred_mean = vec(mean(samples, dims=1))
    pred_std = vec(std(samples, dims=1))
    
    return pred_mean, pred_std, samples
end

# ============================================================================
# Conformal Prediction (Distribution-Free Calibrated Intervals)
# ============================================================================

"""
    ConformalPredictor

Conformal prediction for distribution-free uncertainty quantification.
Provides calibrated prediction intervals with guaranteed coverage.

# Fields
- `model`: Base prediction model (any function X -> y)
- `calibration_scores::Vector{Float64}`: Nonconformity scores from calibration set
- `α::Float64`: Miscoverage level (1-α is coverage probability)
"""
mutable struct ConformalPredictor
    model::Any
    calibration_scores::Vector{Float64}
    α::Float64
end

"""
    ConformalPredictor(model; α=0.1)

Construct a conformal predictor.

# Arguments
- `model`: Base prediction model
- `α::Float64`: Miscoverage level (default: 0.1 for 90% coverage)
"""
function ConformalPredictor(model; α::Float64=0.1)
    return ConformalPredictor(model, Float64[], α)
end

"""
    calibrate_conformal!(cp, X_cal, y_cal)

Calibrate conformal predictor on calibration set.

# Arguments
- `cp::ConformalPredictor`: Conformal predictor
- `X_cal::Matrix`: Calibration inputs
- `y_cal::Vector`: Calibration targets
"""
function calibrate_conformal!(cp::ConformalPredictor, X_cal::AbstractMatrix, y_cal::AbstractVector)
    # Compute predictions on calibration set
    y_pred = cp.model(X_cal)
    
    # Compute nonconformity scores (absolute residuals)
    cp.calibration_scores = abs.(vec(y_pred) .- y_cal)
    
    println("Conformal predictor calibrated with $(length(cp.calibration_scores)) samples")
end

"""
    predict_conformal(cp, X_test)

Predict with conformal prediction intervals.

# Returns
- `y_pred::Vector`: Point predictions
- `lower::Vector`: Lower bounds of prediction intervals
- `upper::Vector`: Upper bounds of prediction intervals
"""
function predict_conformal(cp::ConformalPredictor, X_test::AbstractMatrix)
    # Point predictions
    y_pred = vec(cp.model(X_test))
    
    # Compute quantile of calibration scores
    n_cal = length(cp.calibration_scores)
    q_level = ceil((n_cal + 1) * (1 - cp.α)) / n_cal
    q = quantile(cp.calibration_scores, q_level)
    
    # Prediction intervals
    lower = y_pred .- q
    upper = y_pred .+ q
    
    return y_pred, lower, upper
end

# ============================================================================
# Uncertainty Decomposition (Aleatoric vs Epistemic)
# ============================================================================

"""
    UncertaintyDecomposition

Decompose total uncertainty into aleatoric and epistemic components.

# Fields
- `total::Float64`: Total uncertainty (predictive variance)
- `aleatoric::Float64`: Aleatoric uncertainty (data noise)
- `epistemic::Float64`: Epistemic uncertainty (model uncertainty)
"""
struct UncertaintyDecomposition
    total::Float64
    aleatoric::Float64
    epistemic::Float64
end

"""
    decompose_uncertainty(bnn, X_test)

Decompose uncertainty into aleatoric and epistemic components.

# Arguments
- `bnn::BayesianNN`: Bayesian neural network
- `X_test::Matrix`: Test inputs

# Returns
- `decompositions::Vector{UncertaintyDecomposition}`: Per-sample decomposition
"""
function decompose_uncertainty(bnn::BayesianNN, X_test::AbstractMatrix)
    n_test = size(X_test, 2)
    
    # Get MC samples
    _, _, samples = predict_with_uncertainty(bnn, X_test)
    
    decompositions = UncertaintyDecomposition[]
    
    for i in 1:n_test
        # Total uncertainty: variance of MC samples
        total_var = var(samples[:, i])
        
        # Epistemic uncertainty: variance of means
        # (approximated by variance across MC samples)
        epistemic_var = var(samples[:, i])
        
        # Aleatoric uncertainty: mean of variances
        # (approximated from logvar network)
        logσ² = bnn.logvar_net(X_test[:, i:i])
        aleatoric_var = mean(exp.(logσ²))
        
        # Adjust epistemic to ensure total = aleatoric + epistemic
        epistemic_var = max(0.0, total_var - aleatoric_var)
        
        push!(decompositions, UncertaintyDecomposition(
            sqrt(total_var),
            sqrt(aleatoric_var),
            sqrt(epistemic_var)
        ))
    end
    
    return decompositions
end

# ============================================================================
# Calibration Diagnostics
# ============================================================================

"""
    calibration_curve(y_true, y_pred, y_std; n_bins=10)

Compute calibration curve for uncertainty estimates.

# Arguments
- `y_true::Vector`: True values
- `y_pred::Vector`: Predicted values
- `y_std::Vector`: Predicted standard deviations
- `n_bins::Int`: Number of bins for calibration curve

# Returns
- `expected_probs::Vector`: Expected coverage probabilities
- `observed_probs::Vector`: Observed coverage probabilities
- `ece::Float64`: Expected Calibration Error
"""
function calibration_curve(y_true::AbstractVector, y_pred::AbstractVector, 
                          y_std::AbstractVector; n_bins::Int=10)
    
    # Compute z-scores
    z_scores = abs.((y_true .- y_pred) ./ y_std)
    
    # Expected probabilities (from standard normal CDF)
    expected_probs = range(0.1, 0.9, length=n_bins)
    observed_probs = Float64[]
    
    for p in expected_probs
        # Threshold for this probability level
        threshold = quantile(Normal(0, 1), (1 + p) / 2)
        
        # Observed coverage
        coverage = mean(z_scores .<= threshold)
        push!(observed_probs, coverage)
    end
    
    # Expected Calibration Error (ECE)
    ece = mean(abs.(expected_probs .- observed_probs))
    
    return collect(expected_probs), observed_probs, ece
end

"""
    print_uncertainty_summary(decompositions)

Print summary statistics of uncertainty decomposition.
"""
function print_uncertainty_summary(decompositions::Vector{UncertaintyDecomposition})
    total_unc = [d.total for d in decompositions]
    aleatoric_unc = [d.aleatoric for d in decompositions]
    epistemic_unc = [d.epistemic for d in decompositions]
    
    println("\n" * "="^60)
    println("Uncertainty Decomposition Summary")
    println("="^60)
    println("Total Uncertainty:")
    println("  Mean: $(round(mean(total_unc), digits=4))")
    println("  Std:  $(round(std(total_unc), digits=4))")
    println("\nAleatoric Uncertainty (Data Noise):")
    println("  Mean: $(round(mean(aleatoric_unc), digits=4))")
    println("  Std:  $(round(std(aleatoric_unc), digits=4))")
    println("\nEpistemic Uncertainty (Model Uncertainty):")
    println("  Mean: $(round(mean(epistemic_unc), digits=4))")
    println("  Std:  $(round(std(epistemic_unc), digits=4))")
    println("\nUncertainty Ratio (Epistemic/Total):")
    println("  Mean: $(round(mean(epistemic_unc ./ total_unc), digits=4))")
    println("="^60)
end

end # module
